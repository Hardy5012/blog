{"md5":"e615ba5bca2e39a81e1923b84df47b0a","content":"<nil id=\"nil\">\n\n<div id=\"outline-container-sec-1\" class=\"outline-2\">\n<h2 id=\"sec-1\">Model Representation</h2>\n<div class=\"outline-text-2\" id=\"text-1\">\n</div><div id=\"outline-container-sec-1-1\" class=\"outline-3\">\n<h3 id=\"sec-1-1\">\\text {If network has \\(s_j\\) units in layer \\(j\\) and \\(s_{j+1}\\) units in layer \\(j+1\\), then \\(\\Theta^{(j)}\\) will be of dimension \\(s_{j+1} \\times (s_j + 1)\\).}</h3>\n</div>\n</div>\n<div id=\"outline-container-sec-2\" class=\"outline-2\">\n<h2 id=\"sec-2\">Cost Function</h2>\n<div class=\"outline-text-2\" id=\"text-2\">\n<p class=\"verse\">\nLet&apos;s first define a few variables that we will need to use:<br>\nL = total number of layers in the network<br>\n\\(s_l\\) = number of units (not counting bias unit) in layer l<br>\nK = number of output units/classes<br>\n</p>\n<p>\n\\begin{gather*} J(&#x398;) = - \\frac{1}{m} &#x2211;_{i=1}^m &#x2211;_{k=1}^K \\left[y^{(i)}_k log ((h_&amp;Theta; (x^{(i)}))_k) + (1 - y^{(i)}_k)log (1 - (h_&amp;Theta;(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}&#x2211;_{l=1}^{L-1} &#x2211;_{i=1}^{s_l} &#x2211;_{j=1}^{s_{l+1}} ( &#x398;_{j,i}^{(l)})^2\\end{gather*}\n</p>\n</div>\n</div>\n<div id=\"outline-container-sec-3\" class=\"outline-2\">\n<h2 id=\"sec-3\">Gradient Checking</h2>\n<div class=\"outline-text-2\" id=\"text-3\">\n<p>\nGradient checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with:\n$$\\dfrac{\\partial}{\\partial\\Theta}J(\\Theta) \\approx \\dfrac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)}{2\\epsilon}$$\nHence, we are only adding or subtracting epsilon to the &#x398;j matrix. In octave we can do it as follows:\n</p>\n\n<div class=\"org-src-container\">\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epsilon = 1e-4;\nfor i = 1:n,\n      thetaPlus = theta;\n      thetaPlus(i) += epsilon;\n      thetaMinus = theta;\n      thetaMinus(i) -= epsilon;\n      gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\nend;</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\nOnce you have verified once that your backpropagation algorithm is correct, you don&apos;t need to compute gradApprox again. The code to compute gradApprox can be very slow.\n</p>\n</div>\n</div>\n</nil>\n"}
